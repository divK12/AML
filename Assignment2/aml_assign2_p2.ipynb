{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Assignment-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name                | ID         |\n",
    "|---------------------|------------|\n",
    "| Ananya Sinha        | MDS202307  |\n",
    "| Divyanshi Kumari    | MDS202322  |\n",
    "| Rohit Roy           | MDS202340  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 12:38:03.509915: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-27 12:38:03.524992: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-27 12:38:03.529563: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-27 12:38:03.540736: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-27 12:38:04.271066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.callbacks import ReduceLROnPlateau \n",
    "import random \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the SMS dataframe to a txt file with end character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df = pd.read_csv('data/spam.csv', encoding='ISO-8859-1')\n",
    "sms_df.dropna(how=\"any\", inplace=True, axis=1)\n",
    "sms_df.columns = ['label', 'message']\n",
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the end character you want to add\n",
    "end_character = '¤'\n",
    "\n",
    "# Save messages to a text file with the end character\n",
    "with open('data/sms_messages.txt', 'w') as f:\n",
    "    for message in sms_df['message']:\n",
    "        f.write(f\"{message} {end_character}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the text file into a string \n",
    "with open('data/sms_messages.txt', 'r') as file: \n",
    "    text = file.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '\\x89', '\\x8b', '\\x8e', '£', '¤', '©', 'ª', '¬', '´', '¼', 'Á', 'Â', 'È', 'Ì', 'Ï', 'Ð', 'Ò', 'Ó', 'Ô', 'Õ', 'Û', 'ä', 'å', 'ö', '÷']\n"
     ]
    }
   ],
   "source": [
    "# Storing all the unique characters present in the text \n",
    "vocabulary = sorted(list(set(text))) \n",
    "  \n",
    "# Creating dictionaries to map each character to an index \n",
    "char_to_indices = dict((c, i) for i, c in enumerate(vocabulary)) \n",
    "indices_to_char = dict((i, c) for i, c in enumerate(vocabulary)) \n",
    "  \n",
    "print(vocabulary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "steps = 5\n",
    "sentences = [] \n",
    "next_chars = [] \n",
    "for i in range(0, len(text) - max_length, steps): \n",
    "    sentences.append(text[i: i + max_length]) \n",
    "    next_chars.append(text[i + max_length]) \n",
    "      \n",
    "# Hot encoding each character into a boolean vector \n",
    "X = np.zeros((len(sentences), max_length, len(vocabulary)), dtype = bool) \n",
    "y = np.zeros((len(sentences), len(vocabulary)), dtype = bool) \n",
    "for i, sentence in enumerate(sentences): \n",
    "    for t, char in enumerate(sentence): \n",
    "        X[i, t, char_to_indices[char]] = 1\n",
    "    y[i, char_to_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">278</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">110,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">278</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">117</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,643</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">117</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m278\u001b[0m)            │       \u001b[38;5;34m110,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m278\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m117\u001b[0m)            │        \u001b[38;5;34m32,643\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m117\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">142,731</span> (557.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m142,731\u001b[0m (557.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">142,731</span> (557.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m142,731\u001b[0m (557.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">125,952</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">117</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,093</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">117</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m125,952\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m117\u001b[0m)            │        \u001b[38;5;34m15,093\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m117\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">141,045</span> (550.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m141,045\u001b[0m (550.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">141,045</span> (550.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m141,045\u001b[0m (550.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RNN\n",
    "model_rnn = Sequential() \n",
    "model_rnn.add(SimpleRNN(278, input_shape =(max_length, len(vocabulary))))\n",
    "model_rnn.add(Dropout(0.2))\n",
    "model_rnn.add(Dense(len(vocabulary))) \n",
    "model_rnn.add(Activation('softmax')) \n",
    "optimizer = RMSprop(learning_rate= 0.01) \n",
    "model_rnn.compile(loss ='categorical_crossentropy', optimizer = optimizer) \n",
    "model_rnn.summary()\n",
    "\n",
    "# LSTM\n",
    "model_lstm = Sequential() \n",
    "model_lstm.add(LSTM(128, input_shape =(max_length, len(vocabulary)))) \n",
    "model_lstm.add(Dense(len(vocabulary))) \n",
    "model_lstm.add(Activation('softmax')) \n",
    "optimizer = RMSprop(learning_rate= 0.01) \n",
    "model_lstm.compile(loss ='categorical_crossentropy', optimizer = optimizer) \n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters :\n",
    "\n",
    "RNN : 142,731\n",
    "\n",
    "LSTM : 141,045"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a helper function to save the model after each epoch in which the loss decreases \n",
    "filepath_rnn = \"weights_rnn_2.keras\"\n",
    "checkpoint_rnn = ModelCheckpoint(filepath_rnn, monitor ='loss', \n",
    "                             verbose = 1, save_best_only = True, \n",
    "                             mode ='min')\n",
    "\n",
    "# Defining a helper function to reduce the learning rate each time the learning plateaus \n",
    "reduce_alpha_rnn = ReduceLROnPlateau(monitor ='loss', factor = 0.2, \n",
    "                              patience = 1, min_lr = 0.001) \n",
    "callbacks_rnn = [checkpoint_rnn, reduce_alpha_rnn] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a helper function to save the model after each epoch in which the loss decreases \n",
    "filepath_lstm = \"weights_lstm.keras\"\n",
    "checkpoint_lstm = ModelCheckpoint(filepath_lstm, monitor ='loss', \n",
    "                             verbose = 1, save_best_only = True, \n",
    "                             mode ='min')\n",
    "\n",
    "# Defining a helper function to reduce the learning rate each time the learning plateaus \n",
    "reduce_alpha_lstm = ReduceLROnPlateau(monitor ='loss', factor = 0.2, \n",
    "                              patience = 1, min_lr = 0.001) \n",
    "callbacks_lstm = [checkpoint_lstm, reduce_alpha_lstm] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 4.8622\n",
      "Epoch 1: loss improved from inf to 4.54480, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 4.8614 - learning_rate: 0.0100\n",
      "Epoch 2/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 4.2839\n",
      "Epoch 2: loss improved from 4.54480 to 4.25632, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 4.2838 - learning_rate: 0.0100\n",
      "Epoch 3/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 4.2934\n",
      "Epoch 3: loss did not improve from 4.25632\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 4.2933 - learning_rate: 0.0100\n",
      "Epoch 4/30\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.6083\n",
      "Epoch 4: loss improved from 4.25632 to 3.55393, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 3.6082 - learning_rate: 0.0020\n",
      "Epoch 5/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.5238\n",
      "Epoch 5: loss improved from 3.55393 to 3.50028, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 3.5237 - learning_rate: 0.0020\n",
      "Epoch 6/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.4604\n",
      "Epoch 6: loss improved from 3.50028 to 3.43454, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 3.4604 - learning_rate: 0.0020\n",
      "Epoch 7/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.3605\n",
      "Epoch 7: loss improved from 3.43454 to 3.32021, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 3.3603 - learning_rate: 0.0020\n",
      "Epoch 8/30\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.2352\n",
      "Epoch 8: loss improved from 3.32021 to 3.21620, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 3.2352 - learning_rate: 0.0020\n",
      "Epoch 9/30\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.1586\n",
      "Epoch 9: loss improved from 3.21620 to 3.14711, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 3.1586 - learning_rate: 0.0020\n",
      "Epoch 10/30\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.1219\n",
      "Epoch 10: loss improved from 3.14711 to 3.12232, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 3.1219 - learning_rate: 0.0020\n",
      "Epoch 11/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.1186\n",
      "Epoch 11: loss improved from 3.12232 to 3.11425, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 3.1186 - learning_rate: 0.0020\n",
      "Epoch 12/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.0721\n",
      "Epoch 12: loss improved from 3.11425 to 3.05881, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 3.0721 - learning_rate: 0.0020\n",
      "Epoch 13/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.0321\n",
      "Epoch 13: loss improved from 3.05881 to 3.02910, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 3.0321 - learning_rate: 0.0020\n",
      "Epoch 14/30\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.9863\n",
      "Epoch 14: loss improved from 3.02910 to 2.96623, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.9863 - learning_rate: 0.0020\n",
      "Epoch 15/30\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.9085\n",
      "Epoch 15: loss improved from 2.96623 to 2.89969, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.9085 - learning_rate: 0.0020\n",
      "Epoch 16/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.8682\n",
      "Epoch 16: loss improved from 2.89969 to 2.85123, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.8682 - learning_rate: 0.0020\n",
      "Epoch 17/30\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.8172\n",
      "Epoch 17: loss improved from 2.85123 to 2.80624, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.8172 - learning_rate: 0.0020\n",
      "Epoch 18/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.7814\n",
      "Epoch 18: loss improved from 2.80624 to 2.77535, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.7814 - learning_rate: 0.0020\n",
      "Epoch 19/30\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.7472\n",
      "Epoch 19: loss improved from 2.77535 to 2.74149, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.7472 - learning_rate: 0.0020\n",
      "Epoch 20/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.7174\n",
      "Epoch 20: loss improved from 2.74149 to 2.70372, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.7173 - learning_rate: 0.0020\n",
      "Epoch 21/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.6806\n",
      "Epoch 21: loss improved from 2.70372 to 2.66988, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.6806 - learning_rate: 0.0020\n",
      "Epoch 22/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.6521\n",
      "Epoch 22: loss improved from 2.66988 to 2.63684, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.6520 - learning_rate: 0.0020\n",
      "Epoch 23/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.6184\n",
      "Epoch 23: loss improved from 2.63684 to 2.60720, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.6184 - learning_rate: 0.0020\n",
      "Epoch 24/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.5779\n",
      "Epoch 24: loss improved from 2.60720 to 2.58027, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.5779 - learning_rate: 0.0020\n",
      "Epoch 25/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.5637\n",
      "Epoch 25: loss improved from 2.58027 to 2.57142, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.5637 - learning_rate: 0.0020\n",
      "Epoch 26/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.5615\n",
      "Epoch 26: loss improved from 2.57142 to 2.55737, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.5615 - learning_rate: 0.0020\n",
      "Epoch 27/30\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.5381\n",
      "Epoch 27: loss improved from 2.55737 to 2.53188, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.5381 - learning_rate: 0.0020\n",
      "Epoch 28/30\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.5089\n",
      "Epoch 28: loss improved from 2.53188 to 2.51147, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.5089 - learning_rate: 0.0020\n",
      "Epoch 29/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.4934\n",
      "Epoch 29: loss improved from 2.51147 to 2.49770, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.4934 - learning_rate: 0.0020\n",
      "Epoch 30/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.4842\n",
      "Epoch 30: loss improved from 2.49770 to 2.48209, saving model to weights_rnn_2.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 2.4842 - learning_rate: 0.0020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f1c6817c5f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the RNN model \n",
    "model_rnn.fit(X, y, batch_size = 128, epochs = 30, callbacks = callbacks_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 2.9723\n",
      "Epoch 1: loss improved from inf to 2.62739, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 2.9713 - learning_rate: 0.0100\n",
      "Epoch 2/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 2.2141\n",
      "Epoch 2: loss improved from 2.62739 to 2.16912, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 92ms/step - loss: 2.2140 - learning_rate: 0.0100\n",
      "Epoch 3/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 2.0061\n",
      "Epoch 3: loss improved from 2.16912 to 2.00081, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 93ms/step - loss: 2.0061 - learning_rate: 0.0100\n",
      "Epoch 4/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.8848\n",
      "Epoch 4: loss improved from 2.00081 to 1.88887, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 93ms/step - loss: 1.8848 - learning_rate: 0.0100\n",
      "Epoch 5/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.7856\n",
      "Epoch 5: loss improved from 1.88887 to 1.80539, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 93ms/step - loss: 1.7856 - learning_rate: 0.0100\n",
      "Epoch 6/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.7279\n",
      "Epoch 6: loss improved from 1.80539 to 1.74155, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.7279 - learning_rate: 0.0100\n",
      "Epoch 7/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.6695\n",
      "Epoch 7: loss improved from 1.74155 to 1.69073, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 93ms/step - loss: 1.6696 - learning_rate: 0.0100\n",
      "Epoch 8/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.6202\n",
      "Epoch 8: loss improved from 1.69073 to 1.65154, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 93ms/step - loss: 1.6203 - learning_rate: 0.0100\n",
      "Epoch 9/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.5955\n",
      "Epoch 9: loss improved from 1.65154 to 1.62022, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.5955 - learning_rate: 0.0100\n",
      "Epoch 10/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.5574\n",
      "Epoch 10: loss improved from 1.62022 to 1.59160, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.5575 - learning_rate: 0.0100\n",
      "Epoch 11/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.5400\n",
      "Epoch 11: loss improved from 1.59160 to 1.56648, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 91ms/step - loss: 1.5401 - learning_rate: 0.0100\n",
      "Epoch 12/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.5173\n",
      "Epoch 12: loss improved from 1.56648 to 1.54862, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 93ms/step - loss: 1.5174 - learning_rate: 0.0100\n",
      "Epoch 13/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.4936\n",
      "Epoch 13: loss improved from 1.54862 to 1.52960, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.4937 - learning_rate: 0.0100\n",
      "Epoch 14/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.4727\n",
      "Epoch 14: loss improved from 1.52960 to 1.51281, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.4728 - learning_rate: 0.0100\n",
      "Epoch 15/30\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.4536\n",
      "Epoch 15: loss improved from 1.51281 to 1.50133, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.4537 - learning_rate: 0.0100\n",
      "Epoch 16/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.4446\n",
      "Epoch 16: loss improved from 1.50133 to 1.49113, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 94ms/step - loss: 1.4447 - learning_rate: 0.0100\n",
      "Epoch 17/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.4306\n",
      "Epoch 17: loss improved from 1.49113 to 1.47915, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.4308 - learning_rate: 0.0100\n",
      "Epoch 18/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.4355\n",
      "Epoch 18: loss improved from 1.47915 to 1.47016, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 94ms/step - loss: 1.4356 - learning_rate: 0.0100\n",
      "Epoch 19/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.4098\n",
      "Epoch 19: loss improved from 1.47016 to 1.46049, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 92ms/step - loss: 1.4099 - learning_rate: 0.0100\n",
      "Epoch 20/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.4159\n",
      "Epoch 20: loss improved from 1.46049 to 1.45193, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.4160 - learning_rate: 0.0100\n",
      "Epoch 21/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.4159\n",
      "Epoch 21: loss improved from 1.45193 to 1.44075, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.4160 - learning_rate: 0.0100\n",
      "Epoch 22/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.3912\n",
      "Epoch 22: loss improved from 1.44075 to 1.43349, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 94ms/step - loss: 1.3913 - learning_rate: 0.0100\n",
      "Epoch 23/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.3822\n",
      "Epoch 23: loss improved from 1.43349 to 1.42847, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 94ms/step - loss: 1.3823 - learning_rate: 0.0100\n",
      "Epoch 24/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.3804\n",
      "Epoch 24: loss improved from 1.42847 to 1.42103, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.3805 - learning_rate: 0.0100\n",
      "Epoch 25/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.3757\n",
      "Epoch 25: loss improved from 1.42103 to 1.41307, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.3758 - learning_rate: 0.0100\n",
      "Epoch 26/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.3731\n",
      "Epoch 26: loss improved from 1.41307 to 1.40459, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.3732 - learning_rate: 0.0100\n",
      "Epoch 27/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.3638\n",
      "Epoch 27: loss improved from 1.40459 to 1.39902, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 94ms/step - loss: 1.3639 - learning_rate: 0.0100\n",
      "Epoch 28/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.3489\n",
      "Epoch 28: loss improved from 1.39902 to 1.39107, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 94ms/step - loss: 1.3490 - learning_rate: 0.0100\n",
      "Epoch 29/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.3443\n",
      "Epoch 29: loss improved from 1.39107 to 1.38411, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.3444 - learning_rate: 0.0100\n",
      "Epoch 30/30\n",
      "\u001b[1m723/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.3487\n",
      "Epoch 30: loss improved from 1.38411 to 1.38393, saving model to weights_lstm.keras\n",
      "\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 93ms/step - loss: 1.3488 - learning_rate: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f1ca0cf2210>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the LSTM model \n",
    "model_lstm.fit(X, y, batch_size = 128, epochs = 30, callbacks = callbacks_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lstm_final.pickle']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model_rnn, 'rnn_final.pickle')\n",
    "joblib.dump(model_lstm, 'lstm_final.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to sample an index from a probability array \n",
    "def sample_index(preds, temperature = 1.0): \n",
    "    preds = np.asarray(preds).astype('float64') \n",
    "    preds = np.log(preds) / temperature \n",
    "    exp_preds = np.exp(preds) \n",
    "    preds = exp_preds / np.sum(exp_preds) \n",
    "    probas = np.random.multinomial(1, preds, 1) \n",
    "    return np.argmax(probas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length, max_length, model, diversity): \n",
    "    # Get random starting text \n",
    "    start_index = random.randint(0, len(text) - max_length - 1) \n",
    "    generated = '' \n",
    "    sentence = text[start_index: start_index + max_length]\n",
    "    # sentence = 'sey Devils and the Detroit Red Wings play Ice'\n",
    "    print(\"----Seed SMS----\")\n",
    "    print(sentence)\n",
    "    generated += sentence \n",
    "    next_char = ''\n",
    "    for _ in range(length):\n",
    "        x_pred = np.zeros((1, max_length, len(vocabulary))) \n",
    "        for t, char in enumerate(sentence): \n",
    "            x_pred[0, t, char_to_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose = 0)[0] \n",
    "        next_index = sample_index(preds, diversity)\n",
    "        next_char = indices_to_char[next_index] \n",
    "\n",
    "        generated += next_char \n",
    "        sentence = sentence[1:] + next_char \n",
    "        if next_char == '¤':\n",
    "                break\n",
    "    print(\"----Generated SMS----\")\n",
    "    print(generated) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation by RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Seed SMS----\n",
      "lunch on my way home lor... I tot u dun wan 2 stay in sch today... ¤\n",
      "K then 2marrow are you coming t\n",
      "----Generated SMS----\n",
      "lunch on my way home lor... I tot u dun wan 2 stay in sch today... ¤\n",
      "K then 2marrow are you coming tat and an lore the you alle you low you all you and and in the ane you the to you lore tall an ¤\n"
     ]
    }
   ],
   "source": [
    "# Sample generation by RNN model \n",
    "generate_text(200, max_length, model_rnn, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation by LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Seed SMS----\n",
      "puts things into perspective when something like this happens ¤\n",
      "Now got tv 2 watch meh? U no work to\n",
      "----Generated SMS----\n",
      "puts things into perspective when something like this happens ¤\n",
      "Now got tv 2 watch meh? U no work to drive or something of the we was money and still but if I can still be watch my start so i am late. ¤\n"
     ]
    }
   ],
   "source": [
    "# Sample generation by LSTM model\n",
    "generate_text(200, max_length, model_lstm, 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assigns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
